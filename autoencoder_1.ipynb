{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95bdf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import math\n",
    "import os\n",
    "import pathlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense,ZeroPadding2D, BatchNormalization, Activation, Layer, ReLU, LeakyReLU,Conv2D,AveragePooling2D,UpSampling2D,Reshape,Flatten\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3686a7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dict = {\n",
    "    'Background': '/tmp/all_data/background_for_training.h5',\n",
    "    'Ato4l': '/tmp/all_data/Ato4l_lepFilter_13TeV_filtered.h5',\n",
    "    'hChToTauNu': '/tmp/all_data/hChToTauNu_13TeV_PU20_filtered.h5',\n",
    "    'hToTauTau' : '/tmp/all_data/hToTauTau_13TeV_PU20_filtered.h5', \n",
    "    'leptoquark': '/tmp/all_data/leptoquark_LOWMASS_lepFilter_13TeV_filtered.h5',\n",
    "}\n",
    "\n",
    "background_file = \"/tmp/all_data/background_for_training.h5\" # background data file\n",
    "\n",
    "signal_files = [ # signal data files\n",
    "    '/tmp/all_data/Ato4l_lepFilter_13TeV_filtered.h5',\n",
    "    '/tmp/all_data/hChToTauNu_13TeV_PU20_filtered.h5',\n",
    "    '/tmp/all_data/hToTauTau_13TeV_PU20_filtered.h5', \n",
    "    '/tmp/all_data/leptoquark_LOWMASS_lepFilter_13TeV_filtered.h5',\n",
    "]\n",
    "output_bkg_name = 'output_background'\n",
    "\n",
    "output_signal_names = [ # names for output files\n",
    "    \"output_Ato4l_lepFilter\",\n",
    "    \"output_hChToTauNu\",\n",
    "    \"output_hToTauTau\",\n",
    "    \"output_leptoquark_LOWMASS\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d4ab31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets_convolutional(bkg_file, output_bkg_name, signals_files, output_signal_names, events=None, test_size=0.2, val_size=0.2, input_shape=57):\n",
    "    \n",
    "    # read BACKGROUND data\n",
    "    with h5py.File(bkg_file, 'r') as file:\n",
    "        full_data = file['Particles'][:,:,:-1]\n",
    "        np.random.shuffle(full_data)\n",
    "        if events: full_data = full_data[:events,:,:]\n",
    "    \n",
    "    # define training, test and validation datasets\n",
    "    X_train, X_test = train_test_split(full_data, test_size=test_size, shuffle=True)\n",
    "    X_train, X_val = train_test_split(X_train, test_size=val_size)\n",
    "\n",
    "    del full_data\n",
    "    \n",
    "    with h5py.File(output_bkg_name + '_dataset.h5', 'w') as h5f:\n",
    "        h5f.create_dataset('X_train', data = X_train)\n",
    "        h5f.create_dataset('X_test', data = X_test)\n",
    "        h5f.create_dataset('X_val', data = X_val)\n",
    "    \n",
    "    if signals_files:\n",
    "        # read SIGNAL data\n",
    "        for i, signal_file in enumerate(signals_files):\n",
    "            f = h5py.File(signal_file,'r')\n",
    "            signal_data = f['Particles'][:,:,:-1]\n",
    "            with h5py.File(output_signal_names[i] + '_dataset.h5', 'w') as h5f2:\n",
    "                h5f2.create_dataset('Data', data = signal_data)\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d6f62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_datasets_convolutional(background_file, output_bkg_name, signal_files, output_signal_names, events=None, test_size=0.2, val_size=0.2, input_shape=57)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc41b0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "background = 'BKG_dataset.h5'\n",
    "\n",
    "# load background training data\n",
    "with h5py.File(background, 'r') as file:\n",
    "    X_train = np.array(file['X_train'])\n",
    "    X_test = np.array(file['X_test'])\n",
    "    X_val = np.array(file['X_val'])\n",
    "\n",
    "print(X_train.shape)\n",
    "X_train = np.reshape(X_train, (-1, 19,3,1))\n",
    "X_test = np.reshape(X_test, (-1, 19,3,1))\n",
    "X_val = np.reshape(X_val, (-1, 19,3,1))\n",
    "\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785e5ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_shape = (19,3,1)\n",
    "latent_dimension = 8\n",
    "num_nodes=[16,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca9f743",
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder\n",
    "input_encoder = Input(shape=(image_shape))\n",
    "x = Conv2D(10, kernel_size=(3, 3),\n",
    "         use_bias=False, data_format='channels_last', padding='same')(input_encoder)\n",
    "x = AveragePooling2D(pool_size = (2, 1))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(latent_dimension)(x)\n",
    "enc = Activation('relu')(x)\n",
    "encoder = Model(inputs=input_encoder, outputs=enc)\n",
    "#decoder\n",
    "x = Dense(270)(enc)\n",
    "x = Activation('relu')(x)\n",
    "x = Reshape((9,3,10))(x)\n",
    "x = UpSampling2D((2, 1))(x)\n",
    "x = ZeroPadding2D(((1, 0),(0,0)))(x)\n",
    "x = Conv2D(1, kernel_size=(3,3), use_bias=False, data_format='channels_last', padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "dec = Activation('relu')(x)\n",
    "\n",
    "autoencoder = Model(inputs=input_encoder, outputs=dec)\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b31c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "autoencoder.compile(optimizer = keras.optimizers.Adam(), loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd45917",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "BATCH_SIZE = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80888692",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = autoencoder.fit(X_train, X_train, epochs = EPOCHS, batch_size = BATCH_SIZE,\n",
    "                  validation_data=(X_val, X_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbeb5d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_name = 'CNN_AE'\n",
    "model_directory = 'CNNS/'\n",
    "save_model(model_directory+model_name, autoencoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d6b1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "bkg_prediction = autoencoder.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302d2f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "signals_file = [ # add correct path to signal files (OUTPUTS)\n",
    "    '/tmp/all_data/Ato4l_lepFilter_13TeV_filtered.h5',\n",
    "    '/tmp/all_data/hChToTauNu_13TeV_PU20_filtered.h5',\n",
    "    '/tmp/all_data/hToTauTau_13TeV_PU20_filtered.h5', \n",
    "    '/tmp/all_data/leptoquark_LOWMASS_lepFilter_13TeV_filtered.h5',\n",
    "]\n",
    "signal_labels = [ # names for output files\n",
    "    \"Ato4l_lepFilter\",\n",
    "    \"hChToTauNu\",\n",
    "    \"hToTauTau\",\n",
    "    \"leptoquark_LOWMASS\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c817b931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read signal data\n",
    "signal_data = []\n",
    "for i, label in enumerate(signal_labels):\n",
    "    with h5py.File(signals_file[i], 'r') as file:\n",
    "        test_data = np.array(file[label])\n",
    "    signal_data.append(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df72dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_results = []\n",
    "\n",
    "for i, label in enumerate(signal_labels):\n",
    "    signal_prediction = autoencoder.predict(signal_data[i])\n",
    "    signal_results.append([label, signal_data[i], signal_prediction]) # save [label, true, prediction] for signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7adbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_file = 'cnnvae_results'\n",
    "\n",
    "\n",
    "with h5py.File(save_file, 'w') as file:\n",
    "    file.create_dataset('BKG_input', data=X_test)\n",
    "    file.create_dataset('BKG_predicted', data = bkg_prediction)\n",
    "    for i, sig in enumerate(signal_results):\n",
    "        file.create_dataset('%s_input' %sig[0], data=sig[1])\n",
    "        file.create_dataset('%s_predicted' %sig[0], data=sig[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c14526",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from func import mse_loss\n",
    "\n",
    "# compute loss value (true, predicted)\n",
    "total_loss = []\n",
    "total_loss.append(mse_loss(X_test.reshape((X_test.shape[0],X_test.shape[1]*X_test.shape[2])),\\\n",
    "                           (bkg_prediction.reshape((bkg_prediction.shape[0],bkg_prediction.shape[1]*bkg_prediction.shape[2]))).astype(np.float32)).numpy())\n",
    "for i, signal_X in enumerate(signal_data):\n",
    "    total_loss.append(mse_loss(signal_X.reshape((signal_X.shape[0],signal_X.shape[1]*signal_X.shape[2])),\\\n",
    "                               (signal_results[i][2].reshape((signal_X.shape[0],signal_X.shape[1]*signal_X.shape[2]))).astype(np.float32)).numpy())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972736b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_size=100\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "for i, label in enumerate(signal_labels):\n",
    "    plt.hist(total_loss[i], bins=bin_size, label=label, density = True, histtype='step', fill=False, linewidth=1.5)\n",
    "plt.yscale('log')\n",
    "plt.xlabel(\"Autoencoder Loss\")\n",
    "plt.ylabel(\"Probability (a.u.)\")\n",
    "plt.title('MSE loss')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8613d0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "labels = np.concatenate([['Background'], np.array(signal_labels)])\n",
    "\n",
    "target_background = np.zeros(total_loss[0].shape[0])\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "for i, label in enumerate(labels):\n",
    "    if i == 0: continue # background events\n",
    "    \n",
    "    trueVal = np.concatenate((np.ones(total_loss[i].shape[0]), target_background)) # anomaly=1, bkg=0\n",
    "    predVal_loss = np.concatenate((total_loss[i], total_loss[0]))\n",
    "\n",
    "    fpr_loss, tpr_loss, threshold_loss = roc_curve(trueVal, predVal_loss)\n",
    "\n",
    "    auc_loss = auc(fpr_loss, tpr_loss)\n",
    "    \n",
    "    plt.plot(fpr_loss, tpr_loss, \"-\", label='%s (auc = %.1f%%)'%(label,auc_loss*100.), linewidth=1.5)\n",
    "    \n",
    "    plt.semilogx()\n",
    "    plt.semilogy()\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.legend(loc='center right')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "plt.plot(np.linspace(0, 1),np.linspace(0, 1), '--', color='0.75')\n",
    "plt.axvline(0.00001, color='red', linestyle='dashed', linewidth=1) # threshold value for measuring anomaly detection efficiency\n",
    "plt.title(\"ROC AE\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
