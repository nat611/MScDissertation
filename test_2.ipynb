{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0126ffbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 15:04:26.493787: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, Input\n",
    "from keras.callbacks import EarlyStopping\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e173a390",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dict = { # dictionary for file paths + names \n",
    "    'Background': '/tmp/all_data/background_for_training.h5',\n",
    "    'Ato4l': '/tmp/all_data/Ato4l_lepFilter_13TeV_filtered.h5',\n",
    "    'hChToTauNu': '/tmp/all_data/hChToTauNu_13TeV_PU20_filtered.h5',\n",
    "    'hToTauTau' : '/tmp/all_data/hToTauTau_13TeV_PU20_filtered.h5', \n",
    "    'leptoquark': '/tmp/all_data/leptoquark_LOWMASS_lepFilter_13TeV_filtered.h5',\n",
    "}\n",
    "\n",
    "background_file = \"/tmp/all_data/background_for_training.h5\" # background data file\n",
    "\n",
    "signal_files = [ # signal data files\n",
    "    \n",
    "    '/tmp/all_data/Ato4l_lepFilter_13TeV_filtered.h5',\n",
    "    '/tmp/all_data/hChToTauNu_13TeV_PU20_filtered.h5',\n",
    "    '/tmp/all_data/hToTauTau_13TeV_PU20_filtered.h5', \n",
    "    '/tmp/all_data/leptoquark_LOWMASS_lepFilter_13TeV_filtered.h5',\n",
    "]\n",
    "\n",
    "output_names = [ # names for output files\n",
    "    \"Ato4l_lepFilter\",\n",
    "    \"hChToTauNu\",\n",
    "    \"hToTauTau\",\n",
    "    \"leptoquark_LOWMASS\"\n",
    "]\n",
    "\n",
    "# creating output folders \n",
    "output_folder = '/tmp/all_data/combined_datasets/'\n",
    "model_output = \"/tmp/all_data/models/\"\n",
    "\n",
    "os.makedirs(model_output, exist_ok=True)\n",
    "os.makedirs(output_folder, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed90cb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load particle data from file\n",
    "def load_particles(file_path):\n",
    "    with h5py.File(file_path, 'r') as f:\n",
    "        particles = f['Particles'][:] \n",
    "    return particles\n",
    "\n",
    "def remove_padding(particles): \n",
    "    padding_mask = particles[:, :, 3] != 0  # filter out padding (any index that is not 0) \n",
    "    valid_features = particles[:, :, :3][padding_mask]  # take pt, eta, phi \n",
    "    return valid_features\n",
    "\n",
    "def combine_signal_background(signal_path, background_path, output_path):\n",
    "    # load background particles\n",
    "    background_data = load_particles(background_path)\n",
    "\n",
    "    # load signal particles\n",
    "    signal_data = load_particles(signal_path)\n",
    "\n",
    "    # combine particle data\n",
    "    combined_data = np.concatenate([signal_data, background_data], axis=0)\n",
    "\n",
    "    # save combined dataset\n",
    "    with h5py.File(output_path, 'w') as f:\n",
    "        f.create_dataset('Particles', data=combined_data) # saving new particles data in combined dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbfefec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through and combine each signal with background\n",
    "for output_name, signal_path in zip(output_names, signal_files):\n",
    "    output_path = os.path.join(output_folder, f'{output_name}_with_background.h5')\n",
    "    combine_signal_background(signal_path, background_file, output_path)\n",
    "    print(f\"saved: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c570ad02",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Can't synchronously read data (filter returned failure during read)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m all_features \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, filepath \u001b[38;5;129;01min\u001b[39;00m file_dict\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m----> 4\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mload_particles\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     data \u001b[38;5;241m=\u001b[39m remove_padding(data)\n\u001b[1;32m      6\u001b[0m     all_features\u001b[38;5;241m.\u001b[39mappend(data)\n",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m, in \u001b[0;36mload_particles\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_particles\u001b[39m(file_path):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m h5py\u001b[38;5;241m.\u001b[39mFile(file_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m----> 4\u001b[0m         particles \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mParticles\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m \n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m particles\n",
      "File \u001b[0;32mh5py/_objects.pyx:56\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:57\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.conda/envs/myenv/lib/python3.10/site-packages/h5py/_hl/dataset.py:820\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, args, new_dtype)\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fast_read_ok \u001b[38;5;129;01mand\u001b[39;00m (new_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    819\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 820\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fast_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    822\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Fall back to Python read pathway below\u001b[39;00m\n",
      "File \u001b[0;32mh5py/_selector.pyx:376\u001b[0m, in \u001b[0;36mh5py._selector.Reader.read\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Can't synchronously read data (filter returned failure during read)"
     ]
    }
   ],
   "source": [
    "all_features = []\n",
    "\n",
    "for name, filepath in file_dict.items():\n",
    "    data = load_particles(filepath)\n",
    "    data = remove_padding(data)\n",
    "    all_features.append(data)\n",
    "\n",
    "# combine all datasets into one array\n",
    "combined_features = np.vstack(all_features)\n",
    "\n",
    "# fit scaler to all combined data \n",
    "scaler = StandardScaler() # saved as scaler, can now call this later \n",
    "scaler.fit(combined_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2b26cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_supervised_dataset(bkg_file, signal_file, scaler, output_path, events = None, test_size=0.2, val_size=0.2, input_shape=57, random_num=42):\n",
    "    # load and process background file \n",
    "    with h5py.File(bkg_file, 'r') as file:\n",
    "        bkg_data = file['Particles'][:, :, :-1] # remove the last feature (label)\n",
    "        np.random.shuffle(bkg_data) # shuffle data to remove bias and ensure randomness\n",
    "        if events: # if events not None \n",
    "            bkg_data = bkg_data[:events, :, :]\n",
    "        bkg_data_flattened = bkg_data.reshape(bkg_data.shape[0], input_shape)\n",
    "        y_bkg = np.zeros(bkg_data.shape[0]) #0's for background events\n",
    "\n",
    "    # load and process signal file\n",
    "    with h5py.File(signal_file, 'r') as file:\n",
    "        sig_data = file['Particles'][:, :, :-1] #drop last feature\n",
    "        if events: # if events not None\n",
    "            sig_data = sig_data[:events, :, :] \n",
    "        sig_data_flattened = sig_data.reshape(sig_data.shape[0], input_shape)\n",
    "        y_sig = np.ones(sig_data.shape[0]) # 1's for signal events \n",
    "\n",
    "        # Subsample background to match signal size\n",
    "        n_signal = sig_data.shape[0]\n",
    "        if bkg_data.shape[0] > n_signal:\n",
    "            indices = np.random.choice(bkg_data.shape[0], size=n_signal, replace=False)\n",
    "            bkg_data_balanced = bkg_data[indices]\n",
    "            bkg_data_flat_balanced = bkg_data_flattened[indices]\n",
    "            y_bkg_balanced = y_bkg[indices]\n",
    "        else:\n",
    "            bkg_data_balanced = bkg_data\n",
    "            bkg_data_flat_balanced = bkg_data_flattened\n",
    "            y_bkg_balanced = y_bkg\n",
    "\n",
    "    # Combine balanced datasets\n",
    "    X = np.vstack((bkg_data_balanced, sig_data))  # shape: (n_events, 19, 3)\n",
    "    X_flattened = np.vstack((bkg_data_flat_balanced, sig_data_flattened))  # shape: (n_events, 57)\n",
    "    y = np.concatenate((y_bkg_balanced, y_sig))\n",
    "\n",
    "    # Normalise using provided global scaler\n",
    "    X_flat_scaled = scaler.transform(X_flattened)\n",
    "\n",
    "    # Train/val/test splitting\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_flat_scaled, y, test_size=test_size, stratify=y, random_state=random_num)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=val_size, stratify=y_train, random_state=random_num)\n",
    "\n",
    "    with h5py.File(output_path, 'w') as h5f:\n",
    "        h5f.create_dataset('X_train', data=X_train)\n",
    "        h5f.create_dataset('y_train', data=y_train)\n",
    "        h5f.create_dataset('X_val', data=X_val)\n",
    "        h5f.create_dataset('y_val', data=y_val)\n",
    "        h5f.create_dataset('X_test', data=X_test)\n",
    "        h5f.create_dataset('y_test', data=y_test)\n",
    "        #h5f.create_dataset('X', data=X) # save the raw data (shape: (n_events, 19, 3)) for plotting combined distributions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf7afca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the combined datasets for each signal file \n",
    "for signal_file, output_name in zip(signal_files, output_names):\n",
    "    output_path = f\"{output_folder}/{output_name}_dataset.h5\"\n",
    "    create_supervised_dataset(background_file, signal_file, scaler, output_path, events= 10000, test_size=0.2, val_size=0.2, input_shape=57, random_num=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac9bf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create supervised binary classifier NN \n",
    "def build_model(input_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(input_dim,)))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42adfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for output_name in output_names:\n",
    "    file_path = f\"{output_folder}/{output_name}_dataset.h5\"\n",
    "    with h5py.File(file_path, 'r') as f:\n",
    "        X_train = f['X_train'][:]\n",
    "        y_train = f['y_train'][:]\n",
    "        X_val = f['X_val'][:]\n",
    "        y_val = f['y_val'][:]\n",
    "\n",
    "    model = build_model(input_dim=57)\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "    history = model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                        epochs=50, batch_size=128, callbacks=[early_stop])\n",
    "    \n",
    "    # Save the model\n",
    "    model.save(f\"{model_output}/{output_name}_model.h5\")\n",
    "\n",
    "    # Plot training\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.title(f\"{output_name} Loss\")\n",
    "    plt.grid()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['accuracy'], label='Train Acc')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Acc')\n",
    "    plt.legend()\n",
    "    plt.title(f\"{output_name} Accuracy\")\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8185335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross testing models (ROC CURVES)\n",
    "\n",
    "for test_name in output_names:\n",
    "    with h5py.File(f\"{output_folder}/{test_name}_dataset.h5\", 'r') as f:\n",
    "        X_test = f['X_test'][:]\n",
    "        y_test = f['y_test'][:]\n",
    "\n",
    "    plt.figure(figsize=(8,6))\n",
    "    \n",
    "    for train_name in output_names:\n",
    "        model = load_model(f\"{model_output}/{train_name}_model.h5\")\n",
    "        y_pred = model.predict(X_test).flatten()\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_pred)\n",
    "        auc = roc_auc_score(y_test, y_pred)\n",
    "        plt.plot(fpr, tpr, label=f\"Model [{train_name}] AUC: {auc:.3f}\")\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='Random Guess')\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(f\"ROC Curves: All models tested on [{test_name}] dataset\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
