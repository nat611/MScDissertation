{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0126ffbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.metrics import roc_auc_score, roc_curve\n",
    "# from keras.models import Sequential, load_model\n",
    "# from keras.layers import Dense, Dropout, Input\n",
    "# from keras.callbacks import EarlyStopping\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e173a390",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dict = { # dictionary for file paths + names \n",
    "    'Background': '/home/s1974482/Desktop/MScDissertation/all_data/background_for_training.h5',\n",
    "    'Ato4l': '/home/s1974482/Desktop/MScDissertation/all_data/Ato4l_lepFilter_13TeV_filtered.h5',\n",
    "    'hChToTauNu': '/home/s1974482/Desktop/MScDissertation/all_data/hChToTauNu_13TeV_PU20_filtered.h5',\n",
    "    'hToTauTau' : '/home/s1974482/Desktop/MScDissertation/all_data/hToTauTau_13TeV_PU20_filtered.h5', \n",
    "    'leptoquark': '/home/s1974482/Desktop/MScDissertation/all_data/leptoquark_LOWMASS_lepFilter_13TeV_filtered.h5',\n",
    "}\n",
    "\n",
    "background_file = \"/home/s1974482/Desktop/MScDissertation/all_data/background_for_training.h5\" # background data file\n",
    "\n",
    "signal_files = [ # signal data files\n",
    "    \n",
    "    '/home/s1974482/Desktop/MScDissertation/all_data/Ato4l_lepFilter_13TeV_filtered.h5',\n",
    "    '/home/s1974482/Desktop/MScDissertation/all_data/hChToTauNu_13TeV_PU20_filtered.h5',\n",
    "    '/home/s1974482/Desktop/MScDissertation/all_data/hToTauTau_13TeV_PU20_filtered.h5', \n",
    "    '/home/s1974482/Desktop/MScDissertation/all_data/leptoquark_LOWMASS_lepFilter_13TeV_filtered.h5',\n",
    "]\n",
    "\n",
    "output_names = [ # names for output files\n",
    "    \"Ato4l_lepFilter\",\n",
    "    \"hChToTauNu\",\n",
    "    \"hToTauTau\",\n",
    "    \"leptoquark_LOWMASS\"\n",
    "]\n",
    "\n",
    "# creating output folders \n",
    "output_folder = '/home/s1974482/Desktop/MScDissertation/all_data/combined_datasets/'\n",
    "model_output = \"/home/s1974482/Desktop/MScDissertation/all_data/models/\"\n",
    "\n",
    "os.makedirs(model_output, exist_ok=True)\n",
    "os.makedirs(output_folder, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed90cb30",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# load particle data from file\n",
    "def load_particles(file_path):\n",
    "    with h5py.File(file_path, 'r') as f:\n",
    "        particles = f['Particles'][:] \n",
    "    return particles\n",
    "\n",
    "def remove_padding(particles): \n",
    "    padding_mask = particles[:, :, 3] != 0  # filter out padding (any index that is not 0) \n",
    "    valid_features = particles[:, :, :3][padding_mask]  # take pt, eta, phi \n",
    "    return valid_features\n",
    "\n",
    "def combine_signal_background(signal_path, background_path, output_path):\n",
    "    # load background particles\n",
    "    background_data = load_particles(background_path)\n",
    "\n",
    "    # load signal particles\n",
    "    signal_data = load_particles(signal_path)\n",
    "\n",
    "    # combine particle data\n",
    "    combined_data = np.concatenate([signal_data, background_data], axis=0)\n",
    "\n",
    "    # save combined dataset\n",
    "    with h5py.File(output_path, 'w') as f:\n",
    "        f.create_dataset('Particles', data=combined_data) # saving new particles data in combined dataset\n",
    "\n",
    "# loop through and combine each signal with background\n",
    "for output_name, signal_path in zip(output_names, signal_files):\n",
    "    output_path = os.path.join(output_folder, f'{output_name}_with_background.h5')\n",
    "    combine_signal_background(signal_path, background_file, output_path)\n",
    "    print(f\"saved: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c570ad02",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = []\n",
    "\n",
    "for name, filepath in file_dict.items():\n",
    "    data = load_particles(filepath)\n",
    "    data = data[:, :, :-1]\n",
    "    data= data.reshape(data.shape[0], 57) # flattening the data to 2D to fit scaler\n",
    "    all_features.append(data)\n",
    "\n",
    "# combine all datasets into one array\n",
    "combined_features = np.vstack(all_features)\n",
    "\n",
    "# fit scaler to all combined data \n",
    "scaler = StandardScaler() # saved as scaler, can now call this later \n",
    "scaler.fit(combined_features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
